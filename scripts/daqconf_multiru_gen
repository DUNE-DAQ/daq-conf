#!/usr/bin/env python3
import click
import math
from rich.console import Console
from os.path import exists,abspath,dirname
from pathlib import Path
from daqconf.core.system import System
from daqconf.core.metadata import write_metadata_file
from daqconf.core.sourceid import SourceIDBroker, get_tpg_mode
from daqconf.core.config_file import generate_cli_from_schema
from detchannelmaps._daq_detchannelmaps_py import HardwareMapService
from detdataformats._daq_detdataformats_py import *

console = Console()

# Set moo schema search path
from dunedaq.env import get_moo_model_path
import moo.io
moo.io.default_load_path = get_moo_model_path()

# Load configuration types
import moo.otypes

# Add -h as default help option
CONTEXT_SETTINGS = dict(help_option_names=['-h', '--help'])
@click.command(context_settings=CONTEXT_SETTINGS)
@generate_cli_from_schema('daqconf/confgen.jsonnet', 'daqconf_multiru_gen', 'dataflowapp')
@click.option('--base-command-port', type=int, default=-1, help="Base port of application command endpoints")
@click.option('--hardware-map-file', default='', help="File containing detector hardware map for configuration to run")
@click.option('-s', '--data-rate-slowdown-factor', default=0, help="Scale factor for readout internal clock to generate less data")
@click.option('--enable-dqm', default=False, is_flag=True, help="Enable generation of DQM apps")
@click.option('--op-env', default='', help="Operational environment - used for raw data filename prefix and HDF5 Attribute inside the files")
@click.option('--debug', default=False, is_flag=True, help="Switch to get a lot of printout and dot files")
@click.argument('json_dir', type=click.Path())
def cli(config, base_command_port, hardware_map_file, data_rate_slowdown_factor, enable_dqm, op_env, debug, json_dir):

    output_dir = Path(json_dir)
    if output_dir.exists():
        raise RuntimeError(f"Directory {output_dir} already exists")


    debug_dir = output_dir / 'debug'
    if debug:
        debug_dir.mkdir(parents=True)

    config_data = config[0]
    config_file = config[1]

    if debug:
        console.log(f"Configuration for daqconf: {config_data.pod()}")

    # Get our config objects
    # Loading this one another time... (first time in config_file.generate_cli_from_schema)
    moo.otypes.load_types('daqconf/confgen.jsonnet')
    import dunedaq.daqconf.confgen as confgen

    ## Hack, we shouldn't need to do that, in the future it should be, boot = config_data.boot
    boot = confgen.boot(**config_data.boot)
    if debug: console.log(f"boot configuration object: {boot.pod()}")

    ## etc...
    timing = confgen.timing(**config_data.timing)
    if debug: console.log(f"timing configuration object: {timing.pod()}")

    hsi = confgen.hsi(**config_data.hsi)
    if debug: console.log(f"hsi configuration object: {hsi.pod()}")

    readout = confgen.readout(**config_data.readout)
    if debug: console.log(f"readout configuration object: {readout.pod()}")

    trigger = confgen.trigger(**config_data.trigger)
    if debug: console.log(f"trigger configuration object: {trigger.pod()}")

    dataflow = confgen.dataflow(**config_data.dataflow)
    if debug: console.log(f"dataflow configuration object: {dataflow.pod()}")

    dqm = confgen.dqm(**config_data.dqm)
    if debug: console.log(f"dqm configuration object: {dqm.pod()}")

    dpdk_sender = confgen.dpdk_sender(**config_data.dpdk_sender)
    if debug: console.log(f"dpdk_sender configuration object: {dpdk_sender.pod()}")

    # Update with command-line options
    if base_command_port != -1:
       boot.base_command_port = base_command_port
    if hardware_map_file != '':
        readout.hardware_map_file = hardware_map_file
    if data_rate_slowdown_factor != 0:
        readout.data_rate_slowdown_factor = data_rate_slowdown_factor
    dqm.enable_dqm |= enable_dqm
    if dqm.impl == 'pocket':
        dqm.kafka_address = boot.pocket_url + ":30092"
    if op_env != '':
        boot.op_env = op_env

    console.log("Loading dataflow config generator")
    from daqconf.apps.dataflow_gen import get_dataflow_app
    if dqm.enable_dqm:
        console.log("Loading dqm config generator")
        from daqconf.apps.dqm_gen import get_dqm_app
    console.log("Loading readout config generator")
    from daqconf.apps.readout_gen import get_readout_app
    console.log("Loading trigger config generator")
    from daqconf.apps.trigger_gen import get_trigger_app
    console.log("Loading DFO config generator")
    from daqconf.apps.dfo_gen import get_dfo_app
    console.log("Loading hsi config generator")
    from daqconf.apps.hsi_gen import get_hsi_app
    console.log("Loading fake hsi config generator")
    from daqconf.apps.fake_hsi_gen import get_fake_hsi_app
    console.log("Loading timing partition controller config generator")
    from daqconf.apps.tprtc_gen import get_tprtc_app
    console.log("Loading DPDK sender config generator")
    from daqconf.apps.dpdk_sender_gen import get_dpdk_sender_app
    if trigger.enable_tpset_writing:
        console.log("Loading TPWriter config generator")
        from daqconf.apps.tpwriter_gen import get_tpwriter_app

    sourceid_broker = SourceIDBroker()
    sourceid_broker.debug = debug

    if len(dataflow.apps) == 0:
        console.log(f"No Dataflow apps defined, adding default dataflow0")
        dataflow.apps = [confgen.dataflowapp()]

    host_df = []
    appconfig_df ={}
    df_app_names = []
    for d in dataflow.apps:
        console.log(f"Parsing dataflow app config {d}")

        ## Hack, we shouldn't need to do that, in the future, it should be appconfig = d
        appconfig = confgen.dataflowapp(**d)

        dfapp = appconfig.app_name
        if dfapp in df_app_names:
            appconfig_df[dfapp].update(appconfig)
        else:
            df_app_names.append(dfapp)
            appconfig_df[dfapp] = appconfig
            appconfig_df[dfapp].source_id = sourceid_broker.get_next_source_id("TRBuilder")
            sourceid_broker.register_source_id("TRBuilder", appconfig_df[dfapp].source_id, None)
            host_df += [appconfig.host_df]


    if boot.use_k8s:
        console.log(f'Using k8s')
        trigger.tpset_output_path = abspath(trigger.tpset_output_path)
        for df_app in appconfig_df.values():
            new_output_path = []
            for op in df_app.output_paths:
                new_output_path += [abspath(op)]
            df_app.output_paths = new_output_path
        readout.hardware_map_file = abspath(readout.hardware_map_file)
        readout.data_file = abspath(readout.data_file)
        print(readout.data_file)

    console.log(f"Generating configs for hosts trigger={trigger.host_trigger} DFO={dataflow.host_dfo} dataflow={host_df} hsi={hsi.host_hsi} dqm={dqm.host_dqm}")

    the_system = System(first_port=timing.port_timing+1)

    # Load the hw map file here to extract ru hosts, cards, slr, links, forntend types, sourceIDs and geoIDs
    # The ru apps are determined by the combinations of hostname and card_id, the SourceID determines the
    # DLH (with physical slr+link information), the detId acts as system_type allows to infer the frontend_type
    hw_map_service = HardwareMapService(readout.hardware_map_file)

    # Get the list of RU processes
    dro_infos = hw_map_service.get_all_dro_info()

    tp_mode = get_tpg_mode(readout.enable_firmware_tpg,readout.enable_software_tpg)
    sourceid_broker.register_readout_source_ids(dro_infos, tp_mode)
    sourceid_broker.generate_trigger_source_ids(dro_infos, tp_mode)
    tp_infos = sourceid_broker.get_all_source_ids("Trigger")

    for dro_info in dro_infos:
        console.log(f"Will start a RU process on {dro_info.host} reading card number {dro_info.card}, {len(dro_info.links)} links active")


#    total_number_of_data_producers = 0

#    if use_ssp:
#        total_number_of_data_producers = number_of_data_producers * len(host_ru)
#        console.log(f"Will setup {number_of_data_producers} SSP channels per host, for a total of {total_number_of_data_producers}")
#    else:
#        total_number_of_data_producers = number_of_data_producers * len(host_ru)
#        console.log(f"Will setup {number_of_data_producers} TPC channels per host, for a total of {total_number_of_data_producers}")
#
#    if readout.enable_software_tpg and frontend_type != 'wib':
#        raise Exception("Software TPG is only available for the wib at the moment!")

    if readout.enable_software_tpg and readout.use_fake_data_producers:
        raise Exception("Fake data producers don't support software tpg")

    if readout.use_fake_data_producers and dqm.enable_dqm:
        raise Exception("DQM can't be used with fake data producers")

    if trigger.enable_tpset_writing and not (readout.enable_software_tpg or readout.enable_firmware_tpg):
        raise Exception("TP writing can only be used when either software or firmware TPG is enabled")

    if readout.enable_firmware_tpg and not readout.use_felix:
        raise Exception("firmware TPG can only be used if real felix card is also used.")

    if readout.enable_firmware_tpg and readout.use_fake_data_producers:
        raise Exception("Fake data producers don't support firmware tpg")

#    if (len(region_id) != len(host_ru)) and (len(region_id) != 0):
#        raise Exception("--region-id should be specified once for each --host-ru, or not at all!")

    # TODO, Eric Flumerfelt <eflumerf@github.com> 22-June-2022: Fix if/when multiple frontend types are supported. (Use https://click.palletsprojects.com/en/8.1.x/options/#multi-value-options for RU host/frontend/region config?)
#    if len(region_id) == 0:
#        region_id_temp = []
#        for reg in range(len(host_ru)):
#            region_id_temp.append(reg)
#        region_id = tuple(region_id_temp)

    if hsi.use_hsi_hw and not hsi.hsi_device_name:
        raise Exception("If --use-hsi-hw flag is set to true, --hsi-device-name must be specified!")

    if timing.control_timing_partition and not timing.timing_partition_master_device_name:
        raise Exception("If --control-timing-partition flag is set to true, --timing-partition-master-device-name must be specified!")

    if hsi.control_hsi_hw and not hsi.use_hsi_hw:
        raise Exception("HSI hardware control can only be enabled if HSI hardware is used!")

    if boot.use_k8s and not boot.image:
        raise Exception("You need to provide an --image if running with k8s")

#    host_id_dict = {}
#    ru_configs = []
#    ru_channel_counts = {}
#    for region in region_id: ru_channel_counts[region] = 0
#
#    ru_app_names=[f"ruflx{idx}" if readout.use_felix else f"ruemu{idx}" for idx in range(len(host_ru))]
#    dqm_app_names = [f"dqm{idx}_ru" for idx in range(len(host_ru))]
#
#    for hostidx,ru_host in enumerate(ru_app_names):
#        cardid = 0
#        if host_ru[hostidx] in host_id_dict:
#            host_id_dict[host_ru[hostidx]] = host_id_dict[host_ru[hostidx]] + 1
#            cardid = host_id_dict[host_ru[hostidx]]
#        else:
#            host_id_dict[host_ru[hostidx]] = 0
#        ru_configs.append( {"host": host_ru[hostidx],
#                            "card_id": cardid,
#                            "region_id": region_id[hostidx],
#                            "start_channel": ru_channel_counts[region_id[hostidx]],
#                            "channel_count": number_of_data_producers })
#        ru_channel_counts[region_id[hostidx]] += number_of_data_producers

#    if debug:
#        console.log(f"Output data written to \"{output_path}\"")


    max_expected_tr_sequences = 1
    for df_config in appconfig_df.values():
        if df_config.max_trigger_record_window >= 1:
            df_max_sequences = ((trigger.trigger_window_before_ticks + trigger.trigger_window_after_ticks) / df_config.max_trigger_record_window)
            if df_max_sequences > max_expected_tr_sequences:
                max_expected_tr_sequences = df_max_sequences

    # 11-Jul-2022, KAB: added timeout calculations. The Readout and Trigger App DataRequest timeouts
    # are set based on the command-line parameter that is specified in this script, and they are
    # treated separately here in case we want to customize them somehow in the future.
    # The trigger-record-building timeout is intended to be a multiple of the larger of those two,
    # and it needs to have a non-trivial minimum value.
    # We also include a factor in the TRB timeout that takes into account the number of data producers.
    # At the moment, that factor uses the square root of the number of data producers, and it attempts
    # to take into account the number of data producers in Readout and Trigger.
    MINIMUM_BASIC_TRB_TIMEOUT = 200  # msec
    TRB_TIMEOUT_SAFETY_FACTOR = 2
    DFO_TIMEOUT_SAFETY_FACTOR = 3
    MINIMUM_DFO_TIMEOUT = 10000
    readout_data_request_timeout = boot.data_request_timeout_ms # can that be put somewhere else? in dataflow?
    trigger_data_request_timeout = boot.data_request_timeout_ms
    trigger_record_building_timeout = max(MINIMUM_BASIC_TRB_TIMEOUT, TRB_TIMEOUT_SAFETY_FACTOR * max(readout_data_request_timeout, trigger_data_request_timeout))
    if len(dro_infos) >= 1:
        effective_number_of_data_producers = len(dro_infos)  # number of DataLinkHandlers
        if readout.enable_software_tpg or readout.enable_firmware_tpg:
            effective_number_of_data_producers *= 2  # add in TPSet producers from Trigger (one per Link)
            effective_number_of_data_producers += len(dro_infos)  # add in TA producers from Trigger (one per RU)
        trigger_record_building_timeout = int(math.sqrt(effective_number_of_data_producers) * trigger_record_building_timeout)
    trigger_record_building_timeout += 15 * TRB_TIMEOUT_SAFETY_FACTOR * max_expected_tr_sequences
    dfo_stop_timeout = max(DFO_TIMEOUT_SAFETY_FACTOR * trigger_record_building_timeout, MINIMUM_DFO_TIMEOUT)

    hsi_source_id = sourceid_broker.get_next_source_id("HW_Signals_Interface")
    sourceid_broker.register_source_id("HW_Signals_Interface", hsi_source_id, None)
    if hsi.use_hsi_hw:
        the_system.apps["hsi"] = get_hsi_app(
            CLOCK_SPEED_HZ = readout.clock_speed_hz,
            TRIGGER_RATE_HZ = trigger.trigger_rate_hz,
            CONTROL_HSI_HARDWARE=hsi.control_hsi_hw,
            CONNECTIONS_FILE=hsi.hsi_hw_connections_file,
            READOUT_PERIOD_US = hsi.hsi_readout_period,
            HSI_DEVICE_NAME = hsi.hsi_device_name,
            HSI_ENDPOINT_ADDRESS = hsi.hsi_endpoint_address,
            HSI_ENDPOINT_PARTITION = hsi.hsi_endpoint_partition,
            HSI_RE_MASK=hsi.hsi_re_mask,
            HSI_FE_MASK=hsi.hsi_fe_mask,
            HSI_INV_MASK=hsi.hsi_inv_mask,
            HSI_SOURCE=hsi.hsi_source,
            HSI_SOURCE_ID=hsi_source_id,
            TIMING_PARTITION=timing.timing_partition_name,
            TIMING_HOST=timing.host_timing,
            TIMING_PORT=timing.port_timing,
            HOST=hsi.host_hsi,
            DEBUG=debug)
    else:
        the_system.apps["hsi"] = get_fake_hsi_app(
            CLOCK_SPEED_HZ = readout.clock_speed_hz,
            DATA_RATE_SLOWDOWN_FACTOR = readout.data_rate_slowdown_factor,
            TRIGGER_RATE_HZ = trigger.trigger_rate_hz,
            HSI_SOURCE_ID=hsi_source_id,
            MEAN_SIGNAL_MULTIPLICITY = hsi.mean_hsi_signal_multiplicity,
            SIGNAL_EMULATION_MODE = hsi.hsi_signal_emulation_mode,
            ENABLED_SIGNALS =  hsi.enabled_hsi_signals,
            HOST=hsi.host_hsi,
            DEBUG=debug)

        # the_system.apps["hsi"] = util.App(modulegraph=mgraph_hsi, host=hsi.host_hsi)
    if debug: console.log("hsi cmd data:", the_system.apps["hsi"])

    if timing.control_timing_partition:
        the_system.apps["tprtc"] = get_tprtc_app(
            MASTER_DEVICE_NAME=timing.timing_partition_master_device_name,
            TIMING_PARTITION_ID=timing.timing_partition_id,
            TRIGGER_MASK=timing.timing_partition_trigger_mask,
            RATE_CONTROL_ENABLED=timing.timing_partition_rate_control_enabled,
            SPILL_GATE_ENABLED=timing.timing_partition_spill_gate_enabled,
            TIMING_PARTITION=timing.timing_partition_name,
            TIMING_HOST=timing.host_timing,
            TIMING_PORT=timing.port_timing,
            HOST=timing.host_tprtc,
            DEBUG=debug)

    the_system.apps['trigger'] = get_trigger_app(
        DATA_RATE_SLOWDOWN_FACTOR = readout.data_rate_slowdown_factor,
        CLOCK_SPEED_HZ = readout.clock_speed_hz,
        TP_CONFIG = tp_infos,
        ACTIVITY_PLUGIN = trigger.trigger_activity_plugin,
        ACTIVITY_CONFIG = trigger.trigger_activity_config,
        CANDIDATE_PLUGIN = trigger.trigger_candidate_plugin,
        CANDIDATE_CONFIG = trigger.trigger_candidate_config,
        TTCM_S1=trigger.ttcm_s1,
        TTCM_S2=trigger.ttcm_s2,
        TRIGGER_WINDOW_BEFORE_TICKS = trigger.trigger_window_before_ticks,
        TRIGGER_WINDOW_AFTER_TICKS = trigger.trigger_window_after_ticks,
        HSI_TRIGGER_TYPE_PASSTHROUGH = trigger.hsi_trigger_type_passthrough,
	MLT_BUFFER_TIMEOUT = trigger.mlt_buffer_timeout,
        MLT_MAX_TD_LENGTH_MS = trigger.mlt_max_td_length_ms,
        MLT_SEND_TIMED_OUT_TDS = trigger.mlt_send_timed_out_tds,
        CHANNEL_MAP_NAME = trigger.tpg_channel_map,
        DATA_REQUEST_TIMEOUT=trigger_data_request_timeout,
        HOST=trigger.host_trigger,
        DEBUG=debug)

    the_system.apps['dfo'] = get_dfo_app(
        DF_CONF = appconfig_df,
        STOP_TIMEOUT = dfo_stop_timeout,
        HOST=dataflow.host_dfo,
        DEBUG=debug)


    ru_app_names=[]
    dqm_app_names = []
    for dro_idx,dro_config in enumerate(dro_infos):
        host=dro_config.host.replace("-","")
        ru_name = f"ru{host}{dro_config.card}"
        ru_app_names.append(ru_name)

        numa_id = readout.numa_config['default_id']
        latency_numa = readout.numa_config['default_latency_numa_aware']
        latency_preallocate = readout.numa_config['default_latency_preallocation']
        card_override = -1
        for ex in readout.numa_config['exceptions']:
            if ex['host'] == dro_config.host and ex['card'] == dro_config.card:
                numa_id = ex['numa_id']
                latency_numa = ex['latency_buffer_numa_aware']
                latency_preallocate = ex['latency_buffer_preallocation']
                card_override = ex['felix_card_id']

        the_system.apps[ru_name] = get_readout_app(
            HOST=dro_config.host,
            DRO_CONFIG=dro_config,
            EMULATOR_MODE = readout.emulator_mode,
            DATA_RATE_SLOWDOWN_FACTOR = readout.data_rate_slowdown_factor,
            DATA_FILE = readout.data_file,
            FLX_INPUT = readout.use_felix,
            CLOCK_SPEED_HZ = readout.clock_speed_hz,
            RAW_RECORDING_ENABLED = readout.enable_raw_recording,
            RAW_RECORDING_OUTPUT_DIR = readout.raw_recording_output_dir,
            SOFTWARE_TPG_ENABLED = readout.enable_software_tpg,
            FIRMWARE_TPG_ENABLED = readout.enable_firmware_tpg,
            DTP_CONNECTIONS_FILE= readout.dtp_connections_file,
            FIRMWARE_HIT_THRESHOLD= readout.firmware_hit_threshold,
            TPG_CHANNEL_MAP = trigger.tpg_channel_map,
            USE_FAKE_DATA_PRODUCERS = readout.use_fake_data_producers,
            LATENCY_BUFFER_SIZE=readout.latency_buffer_size,
            DATA_REQUEST_TIMEOUT=readout_data_request_timeout,
            FRAGMENT_SEND_TIMEOUT=readout.fragment_send_timeout_ms,
            SOURCEID_BROKER = sourceid_broker,
            READOUT_SENDS_TP_FRAGMENTS = readout.readout_sends_tp_fragments,
            ENABLE_DPDK_SENDER=dpdk_sender.enable_dpdk_sender,
            ENABLE_DPDK_READER=readout.enable_dpdk_reader,
            EAL_ARGS=readout.eal_args,
            BASE_SOURCE_IP=readout.base_source_ip,
            DESTINATION_IP=readout.destination_ip,
            NUMA_ID = numa_id,
            LATENCY_BUFFER_NUMA_AWARE = latency_numa,
            LATENCY_BUFFER_ALLOCATION_MODE = latency_preallocate,
            CARD_ID_OVERRIDE = card_override,
            DEBUG=debug)

        if boot.use_k8s:
            if readout.use_felix:
                c = card_override if card_override != -1 else dro_config.card
                the_system.apps[ru_name].resources = {
                    f"felix.cern/flx{c}-data": "1", # requesting FLX{c}
                    "memory": "32Gi" # yes bro
                }
            # TODO: HACK, can't do that any other way now, please give me a nice asset manager
            the_system.apps[ru_name].mounted_dirs += [{
                'name': 'frames-bin',
                'physical_location': dirname(readout.data_file),
                'in_pod_location':   dirname(readout.data_file),
                'read_only': True,
            }]


        if debug:
            console.log(f"{ru_name} app: {the_system.apps[ru_name]}")

        if dqm.enable_dqm:
            dqm_name = "dqm" + ru_name
            dqm_app_names.append(dqm_name)
            dqm_links = [link.dro_source_id for link in dro_config.links]
            the_system.apps[dqm_name] = get_dqm_app(
                DQM_IMPL=dqm.impl,
                DATA_RATE_SLOWDOWN_FACTOR=readout.data_rate_slowdown_factor,
                CLOCK_SPEED_HZ=readout.clock_speed_hz,
                MAX_NUM_FRAMES=dqm.max_num_frames,
                DQMIDX = dro_idx,
                KAFKA_ADDRESS=dqm.kafka_address,
                KAFKA_TOPIC=dqm.kafka_topic,
                CMAP=dqm.cmap,
                RAW_PARAMS=dqm.raw_params,
                RMS_PARAMS=dqm.rms_params,
                STD_PARAMS=dqm.std_params,
                FOURIER_CHANNEL_PARAMS=dqm.fourier_channel_params,
                FOURIER_PLANE_PARAMS=dqm.fourier_plane_params,
                LINKS=dqm_links,
                HOST=dqm.host_dqm[dro_idx % len(dqm.host_dqm)],
                DRO_CONFIG=dro_config,
                DEBUG=debug)

            if debug: console.log(f"{dqm_name} app: {the_system.apps[dqm_name]}")

    dqm_df_app_names = []
    idx = 0

    for app_name,df_config in appconfig_df.items():
        dfidx = df_config.source_id
        the_system.apps[app_name] = get_dataflow_app(
            HOSTIDX=dfidx,
            OUTPUT_PATHS = df_config.output_paths,
            APP_NAME=app_name,
            OPERATIONAL_ENVIRONMENT = boot.op_env,
            DATA_STORE_MODE=df_config.data_store_mode,
            MAX_FILE_SIZE = df_config.max_file_size,
            MAX_TRIGGER_RECORD_WINDOW = df_config.max_trigger_record_window,
            MAX_EXPECTED_TR_SEQUENCES = max_expected_tr_sequences,
            TOKEN_COUNT = df_config.token_count,
            TRB_TIMEOUT = trigger_record_building_timeout,
            HOST=df_config.host_df,
            HAS_DQM=dqm.enable_dqm,
            HARDWARE_MAP_FILE=readout.hardware_map_file,
            DEBUG=debug
        )
        if boot.use_k8s:
            the_system.apps[app_name].mounted_dirs += [{
                'name': f'raw-data-{i}',
                'physical_location': opath,
                'in_pod_location': opath,
                'read_only': False,
            } for i,opath in enumerate(set(df_config.output_paths))]
            # doubling down on ugly hacking...
            if dirname(readout.hardware_map_file) not in df_config.output_paths:
                the_system.apps[app_name].mounted_dirs += [{
                    'name': 'hardware-map',
                    'physical_location': dirname(readout.hardware_map_file),
                    'in_pod_location': dirname(readout.hardware_map_file),
                    'read_only': True,
                }]


        if dqm.enable_dqm:
            dqm_name = f"dqmdf{dfidx}"
            dqm_df_app_names.append(dqm_name)
            dqm_links = [link.dro_source_id for dro_config in dro_infos for link in dro_config.links]
            the_system.apps[dqm_name] = get_dqm_app(
                DQM_IMPL=dqm.impl,
                DATA_RATE_SLOWDOWN_FACTOR = readout.data_rate_slowdown_factor,
                CLOCK_SPEED_HZ = readout.clock_speed_hz,
                MAX_NUM_FRAMES=dqm.max_num_frames,
                DQMIDX = dfidx,
                KAFKA_ADDRESS=dqm.kafka_address,
                KAFKA_TOPIC=dqm.kafka_topic,
                CMAP=dqm.cmap,
                RAW_PARAMS=[0, 0],
                RMS_PARAMS=[0, 0],
                STD_PARAMS=[0, 0],
                FOURIER_CHANNEL_PARAMS=[0, 0],
                FOURIER_PLANE_PARAMS=[0, 0],
                LINKS=dqm_links,
                HOST=dqm.host_dqm[idx%len(dqm.host_dqm)],
                MODE='df',
                DF_RATE=dqm.df_rate * len(host_df),
                DF_ALGS=dqm.df_algs,
                DF_TIME_WINDOW=trigger.trigger_window_before_ticks + trigger.trigger_window_after_ticks,
                DRO_CONFIG=dro_config, # This is coming from the readout loop
                DEBUG=debug)

            if debug: console.log(f"{dqm_name} app: {the_system.apps[dqm_name]}")
        idx += 1

    if trigger.enable_tpset_writing:
        tpw_name=f'tpwriter'
        dfidx = sourceid_broker.get_next_source_id("TRBuilder")
        sourceid_broker.register_source_id("TRBuilder", dfidx, None)
        the_system.apps[tpw_name] = get_tpwriter_app(
            OUTPUT_PATH = trigger.tpset_output_path,
            APP_NAME = tpw_name,
            OPERATIONAL_ENVIRONMENT = boot.op_env,
            MAX_FILE_SIZE = trigger.tpset_output_file_size,
            DATA_RATE_SLOWDOWN_FACTOR = readout.data_rate_slowdown_factor,
            CLOCK_SPEED_HZ = readout.clock_speed_hz,
            HARDWARE_MAP_FILE=readout.hardware_map_file,
            SOURCE_IDX=dfidx,
            FIRMWARE_TPG_ENABLED=readout.enable_firmware_tpg,
            HOST=trigger.host_tpw,
            DEBUG=debug)
        if boot.use_k8s: ## TODO schema
            the_system.apps[tpw_name].mounted_dirs += [{
                'name': 'raw-data',
                'physical_location':trigger.tpset_output_path,
                'in_pod_location':trigger.tpset_output_path,
                'read_only': False
            }]

        if debug: console.log(f"{tpw_name} app: {the_system.apps[tpw_name]}")

    all_apps_except_ru = []
    all_apps_except_ru_and_df = []

    if dpdk_sender.enable_dpdk_sender:
        the_system.apps["dpdk_sender"] = get_dpdk_sender_app(
            HOST=dpdk_sender.host_dpdk_sender[0],
        )

    for name,app in the_system.apps.items():
        if app.name=="__app":
            app.name=name

        if app.name not in ru_app_names:
            all_apps_except_ru += [app]
        if app.name not in ru_app_names+df_app_names:
            all_apps_except_ru_and_df += [name]

        # HACK
        boot_order = ru_app_names + df_app_names + [app for app in all_apps_except_ru_and_df]
        if debug:
            console.log(f'Boot order: {boot_order}')

    #     console.log(f"MDAapp config generated in {json_dir}")
    from daqconf.core.conf_utils import make_app_command_data
    from daqconf.core.fragment_producers import  connect_all_fragment_producers, set_mlt_links, remove_mlt_link

    if debug:
        the_system.export(debug_dir / "system_no_frag_prod_connection.dot")
    connect_all_fragment_producers(the_system, verbose=debug)

    # console.log("After connecting fragment producers, trigger mgraph:", the_system.apps['trigger'].modulegraph)
    # console.log("After connecting fragment producers, the_system.app_connections:", the_system.app_connections)

    set_mlt_links(the_system, "trigger", verbose=debug)

    mlt_links=the_system.apps["trigger"].modulegraph.get_module("mlt").conf.links
    if debug:
        console.log(f"After set_mlt_links, mlt_links is {mlt_links}")

    # HACK HACK HACK P. Rodrigues 2022-03-04 We decided not to request
    # TPs from readout for the 2.10 release. It would be nice to
    # achieve this by just not adding fragment producers for the
    # relevant links in readout_gen.py, but then the necessary input
    # and output queues for the DataLinkHandler modules are not
    # created. So instead we do it this roundabout way: the fragment
    # producers are all created, they are added to the MLT's list of
    # links to read out from (in set_mlt_links above), and then
    # removed here. We rely on a convention that TP links have element
    # value >= 1000.
    #
    # This code should be removed after 2.10, when we will have
    # decided how to handle raw TP data as fragments
#    for link in mlt_links:
#        if link["subsystem"] == system_type and link["element"] >= 1000:
#            remove_mlt_link(the_system, link)

    mlt_links=the_system.apps["trigger"].modulegraph.get_module("mlt").conf.links
    if debug:
        console.log(f"After remove_mlt_links, mlt_links is {mlt_links}")
    # END HACK

    if debug:
        the_system.export(debug_dir / "system.dot")

    ####################################################################
    # Application command data generation
    ####################################################################

    # Arrange per-app command data into the format used by util.write_json_files()
    app_command_datas = {
        name : make_app_command_data(the_system, app,name, verbose=debug, use_k8s=boot.use_k8s)
        for name,app in the_system.apps.items()
    }

    ##################################################################################

    # Make boot.json config
    from daqconf.core.conf_utils import make_system_command_datas,generate_boot, write_json_files

    # HACK: Make sure RUs start after trigger
    forced_deps = []

    for i,host in enumerate(dro_infos):
        ru_name = ru_app_names[i]
        forced_deps.append(['hsi', ru_name])
        if trigger.enable_tpset_writing:
            forced_deps.append(['tpwriter', ru_name])

    if dqm.enable_dqm:
        for i,host in enumerate(dro_infos):
            dqm_name = dqm_app_names[i]
            forced_deps.append([dqm_name, 'dfo'])
        for i,host in enumerate(host_df):
            dqm_name = dqm_df_app_names[i]
            forced_deps.append([dqm_name, 'dfo'])
    forced_deps.append(['trigger','hsi'])

    system_command_datas = make_system_command_datas(
        boot,
        the_system,
        forced_deps,
        verbose=debug
    )


    if readout.thread_pinning_file != "":
        import os
        resolved_thread_pinning_file = os.path.abspath(os.path.expanduser(os.path.expandvars(readout.thread_pinning_file)))
        if not exists(resolved_thread_pinning_file):
            raise RuntimeError(f'Cannot find the file {readout.thread_pinning_file} ({resolved_thread_pinning_file})')
    
        system_command_datas['boot']['scripts'] = {
            "thread_pinning": {
                "cmd": [
                    "readout-affinity.py --pinfile ${DUNEDAQ_THREAD_PIN_FILE}"
                ],
                "env": {
                    "DUNEDAQ_THREAD_PIN_FILE": resolved_thread_pinning_file,
                    "LD_LIBRARY_PATH": "getenv",
                    "PATH": "getenv"
                }
            }
        }


    write_json_files(app_command_datas, system_command_datas, output_dir, verbose=debug)

    console.log(f"MDAapp config generated in {output_dir}")

    write_metadata_file(output_dir, "daqconf_multiru_gen", config_file)
    import json
    hwmap_file = open(readout.hardware_map_file, 'r')
    hwmap_data = hwmap_file.read()
    with open(output_dir/'hwmap.json', 'w') as f :
        json.dump({
            '__to_read_(rm_backslashes)': f"python -c \"import json,sys;print(json.load(open(sys.argv[1]))['hwmap'])\" {output_dir}/hwmap.json",
            'hwmap': hwmap_data
        }, f, indent=2)

    if debug:
        for name in the_system.apps:
            the_system.apps[name].export(debug_dir / f"{name}.dot")

if __name__ == '__main__':
    try:
        cli(show_default=True, standalone_mode=True)
    except Exception as e:
        console.print_exception()
