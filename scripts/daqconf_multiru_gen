#!/usr/bin/env python3
import json
import os
import math
import sys
import glob
import rich.traceback
from rich.console import Console
from collections import defaultdict
from os.path import exists, join
from daqconf.core.system import System
from daqconf.core.metadata import write_metadata_file
from daqconf.core.sourceid import *
import random
import string

from detchannelmaps._daq_detchannelmaps_py import *

console = Console()

# Set moo schema search path
from dunedaq.env import get_moo_model_path
import moo.io
moo.io.default_load_path = get_moo_model_path()

# Load configuration types
import moo.otypes

moo.otypes.load_types('daqconf/confgen.jsonnet')
import dunedaq.daqconf.confgen as confgen

import click

def configure(ctx, param, filename):
    if exists(filename):
        ctx.default_map = JSON.load(filename)

# Add -h as default help option
CONTEXT_SETTINGS = dict(help_option_names=['-h', '--help'], default_map=confgen.Config().pod())
@click.command(context_settings=CONTEXT_SETTINGS)
@click.option(
    '-c', '--config',
    type         = click.Path(dir_okay=False),
    default      = "",
    callback     = configure,
    is_eager     = True,
    expose_value = False,
    help         = 'Read option defaults from the specified JSON file',
    show_default = True,
)
@click.option('--base-command-port', type=int, default=3333, help="Base port of application command endpoints")
@click.option('--hardware-map-file', default='./HardwareMap.txt', help="File containing detector hardware map for configuration to run")
@click.option('--data-rate-slowdown-factor', default=1)
@click.option('-t', '--trigger-rate-hz', default=1.0, help='Fake HSI only: rate at which fake HSIEvents are sent. 0 - disable HSIEvent generation.')
@click.option('-f', '--use-felix', is_flag=True, help="Use real felix cards instead of fake ones")
@click.option('--op-env', default='swtest', help="Operational environment - used for raw data filename prefix and HDF5 Attribute inside the files")
@click.option('--debug', default=False, is_flag=True, help="Switch to get a lot of printout and dot files")
@click.argument('json_dir', type=click.Path())
@click.pass_context
def cli(ctx, base_command_port, hardware_map_file, data_rate_slowdown_factor, trigger_rate_hz, use_felix, op_env, debug, json_dir):
    if exists(json_dir):
        raise RuntimeError(f"Directory {json_dir} already exists")

    if debug:
        console.log(f"Configuration for daqconf: {ctx.default_map}")
    globals().update(ctx.default_map)

    console.log("Loading dataflow config generator")
    from daqconf.apps.dataflow_gen import get_dataflow_app
    if enable_dqm:
        console.log("Loading dqm config generator")
        from daqconf.apps.dqm_gen import get_dqm_app
    console.log("Loading readout config generator")
    from daqconf.apps.readout_gen import get_readout_app
    console.log("Loading trigger config generator")
    from daqconf.apps.trigger_gen import get_trigger_app
    console.log("Loading DFO config generator")
    from daqconf.apps.dfo_gen import get_dfo_app
    console.log("Loading hsi config generator")
    from daqconf.apps.hsi_gen import get_hsi_app
    console.log("Loading fake hsi config generator")
    from daqconf.apps.fake_hsi_gen import get_fake_hsi_app
    console.log("Loading timing partition controller config generator")
    from daqconf.apps.tprtc_gen import get_tprtc_app
    if enable_tpset_writing:
        console.log("Loading TPWriter config generator")
        from daqconf.apps.tpwriter_gen import get_tpwriter_app

    host_df = []
    appconfig_df ={}
    token_count = 0
    for k,v in ctx.default_map.items():
        if "dataflow." in k:
            appconfig_df[k[9:]] = v
            token_count += v['token_count']
            host_df += [v['host_df']]

    console.log(f"Generating configs for hosts trigger={host_trigger} DFO={host_dfo} dataflow={host_df} hsi={host_hsi} dqm={host_dqm}")

    the_system = System(first_port=port_timing+1)

    # Load the hw map file here to extract ru hosts, cards, slr, links, forntend types, sourceIDs and geoIDs
    # The ru apps are determined by the combinations of hostname and card_id, the SourceID determines the 
    # DLH (with physical slr+link information), the detId acts as system_type allows to infer the frontend_type
    hw_map_service = HardwareMapService(hardware_map_file)

    # Get the list of RU processes
    dro_infos = hw_map_service.get_all_dro_info()

    sourceid_broker = SourceIDBroker()
    sourceid_broker.debug = debug
    sourceid_broker.register_readout_source_ids(dro_infos)
    tp_mode = get_tpg_mode(enable_firmware_tpg,enable_software_tpg)
    sourceid_broker.generate_trigger_source_ids(dro_infos, tp_mode)
    tp_infos = sourceid_broker.get_all_source_ids("Trigger")

    for dro_info in dro_infos:
        console.log(f"Will start a RU process on {dro_info.host} reading card number {dro_info.card}, {len(dro_info.links)} links active")


#    total_number_of_data_producers = 0

#    if use_ssp:
#        total_number_of_data_producers = number_of_data_producers * len(host_ru)
#        console.log(f"Will setup {number_of_data_producers} SSP channels per host, for a total of {total_number_of_data_producers}")
#    else:
#        total_number_of_data_producers = number_of_data_producers * len(host_ru)
#        console.log(f"Will setup {number_of_data_producers} TPC channels per host, for a total of {total_number_of_data_producers}")
#
#    if enable_software_tpg and frontend_type != 'wib':
#        raise Exception("Software TPG is only available for the wib at the moment!")

    if enable_software_tpg and use_fake_data_producers:
        raise Exception("Fake data producers don't support software tpg")

    if use_fake_data_producers and enable_dqm:
        raise Exception("DQM can't be used with fake data producers")

    if enable_tpset_writing and not (enable_software_tpg or enable_firmware_tpg):
        raise Exception("TP writing can only be used when either software or firmware TPG is enabled")

    if enable_firmware_tpg and not use_felix:
        raise Exception("firmware TPG can only be used if real felix card is also used.")

    if enable_firmware_tpg and use_fake_data_producers:
        raise Exception("Fake data producers don't support firmware tpg")

    if token_count > 0:
        trigemu_token_count = token_count
    else:
        trigemu_token_count = 0

#    if (len(region_id) != len(host_ru)) and (len(region_id) != 0):
#        raise Exception("--region-id should be specified once for each --host-ru, or not at all!")

    # TODO, Eric Flumerfelt <eflumerf@github.com> 22-June-2022: Fix if/when multiple frontend types are supported. (Use https://click.palletsprojects.com/en/8.1.x/options/#multi-value-options for RU host/frontend/region config?)
#    if len(region_id) == 0:
#        region_id_temp = []
#        for reg in range(len(host_ru)):
#            region_id_temp.append(reg)
#        region_id = tuple(region_id_temp)

    if use_hsi_hw and not hsi_device_name:
        raise Exception("If --use-hsi-hw flag is set to true, --hsi-device-name must be specified!")

    if control_timing_partition and not timing_partition_master_device_name:
        raise Exception("If --control-timing-partition flag is set to true, --timing-partition-master-device-name must be specified!")

    if control_hsi_hw and not use_hsi_hw:
        raise Exception("HSI hardware control can only be enabled if HSI hardware is used!")

    if opmon_impl == 'cern':
        info_svc_uri = "kafka://monkafka.cern.ch:30092/opmon"
    elif opmon_impl == 'pocket':
        info_svc_uri = "kafka://" + pocket_url + ":30092/opmon"
    else:
        info_svc_uri = "file://info_{APP_NAME}_{APP_PORT}.json"

    if use_k8s and not image:
        raise Exception("You need to provide an --image if running with k8s")

    ers_settings=dict()

    if ers_impl == 'cern':
        use_kafka = True
        ers_settings["INFO"] =    "erstrace,throttle,lstdout,erskafka(monkafka.cern.ch:30092)"
        ers_settings["WARNING"] = "erstrace,throttle,lstdout,erskafka(monkafka.cern.ch:30092)"
        ers_settings["ERROR"] =   "erstrace,throttle,lstdout,erskafka(monkafka.cern.ch:30092)"
        ers_settings["FATAL"] =   "erstrace,lstdout,erskafka(monkafka.cern.ch:30092)"
    elif ers_impl == 'pocket':
        use_kafka = True
        ers_settings["INFO"] =    "erstrace,throttle,lstdout,erskafka(" + pocket_url + ":30092)"
        ers_settings["WARNING"] = "erstrace,throttle,lstdout,erskafka(" + pocket_url + ":30092)"
        ers_settings["ERROR"] =   "erstrace,throttle,lstdout,erskafka(" + pocket_url + ":30092)"
        ers_settings["FATAL"] =   "erstrace,lstdout,erskafka(" + pocket_url + ":30092)"
    else:
        use_kafka = False
        ers_settings["INFO"] =    "erstrace,throttle,lstdout"
        ers_settings["WARNING"] = "erstrace,throttle,lstdout"
        ers_settings["ERROR"] =   "erstrace,throttle,lstdout"
        ers_settings["FATAL"] =   "erstrace,lstdout"

    dqm_kafka_address = "monkafka.cern.ch:30092" if dqm_impl == 'cern' else pocket_url + ":30092" if dqm_impl == 'pocket' else ''

#    host_id_dict = {}
#    ru_configs = []
#    ru_channel_counts = {}
#    for region in region_id: ru_channel_counts[region] = 0
#
#    ru_app_names=[f"ruflx{idx}" if use_felix else f"ruemu{idx}" for idx in range(len(host_ru))]
#    dqm_app_names = [f"dqm{idx}_ru" for idx in range(len(host_ru))]
#
#    for hostidx,ru_host in enumerate(ru_app_names):
#        cardid = 0
#        if host_ru[hostidx] in host_id_dict:
#            host_id_dict[host_ru[hostidx]] = host_id_dict[host_ru[hostidx]] + 1
#            cardid = host_id_dict[host_ru[hostidx]]
#        else:
#            host_id_dict[host_ru[hostidx]] = 0
#        ru_configs.append( {"host": host_ru[hostidx],
#                            "card_id": cardid,
#                            "region_id": region_id[hostidx],
#                            "start_channel": ru_channel_counts[region_id[hostidx]],
#                            "channel_count": number_of_data_producers })
#        ru_channel_counts[region_id[hostidx]] += number_of_data_producers

#    if debug:
#        console.log(f"Output data written to \"{output_path}\"")

#    if use_k8s and output_path == '.':
#        output_path = os.getcwd()

    max_expected_tr_sequences = 1
    for df_config in appconfig_df.values():
        if df_config['max_trigger_record_window'] >= 1:
            df_max_sequences = ((trigger_window_before_ticks + trigger_window_after_ticks) / df_config['max_trigger_record_window'])
            if df_max_sequences > max_expected_tr_sequences:
                max_expected_tr_sequences = df_max_sequences

    # 11-Jul-2022, KAB: added timeout calculations. The Readout and Trigger App DataRequest timeouts
    # are set based on the command-line parameter that is specified in this script, and they are
    # treated separately here in case we want to customize them somehow in the future.
    # The trigger-record-building timeout is intended to be a multiple of the larger of those two,
    # and it needs to have a non-trivial minimum value.
    # We also include a factor in the TRB timeout that takes into account the number of data producers.
    # At the moment, that factor uses the square root of the number of data producers, and it attempts
    # to take into account the number of data producers in Readout and Trigger.
    MINIMUM_BASIC_TRB_TIMEOUT = 200  # msec
    TRB_TIMEOUT_SAFETY_FACTOR = 2
    DFO_TIMEOUT_SAFETY_FACTOR = 3
    MINIMUM_DFO_TIMEOUT = 10000
    readout_data_request_timeout = data_request_timeout_ms
    trigger_data_request_timeout = data_request_timeout_ms
    trigger_record_building_timeout = max(MINIMUM_BASIC_TRB_TIMEOUT, TRB_TIMEOUT_SAFETY_FACTOR * max(readout_data_request_timeout, trigger_data_request_timeout))
    if len(dro_infos) >= 1:
        effective_number_of_data_producers = len(dro_infos)  # number of DataLinkHandlers
        if enable_software_tpg or enable_firmware_tpg:
            effective_number_of_data_producers *= 2  # add in TPSet producers from Trigger (one per Link)
            effective_number_of_data_producers += len(dro_infos)  # add in TA producers from Trigger (one per RU)
        trigger_record_building_timeout = int(math.sqrt(effective_number_of_data_producers) * trigger_record_building_timeout)
    trigger_record_building_timeout += 15 * TRB_TIMEOUT_SAFETY_FACTOR * max_expected_tr_sequences
    dfo_stop_timeout = max(DFO_TIMEOUT_SAFETY_FACTOR * trigger_record_building_timeout, MINIMUM_DFO_TIMEOUT)

    hsi_source_id = sourceid_broker.get_next_source_id("HW_Signals_Interface")
    sourceid_broker.register_source_id(hsi_source_id, "HW_Signals_Interface", None)
    if use_hsi_hw:
        the_system.apps["hsi"] = get_hsi_app(
            CLOCK_SPEED_HZ = clock_speed_hz,
            TRIGGER_RATE_HZ = trigger_rate_hz,
            CONTROL_HSI_HARDWARE=control_hsi_hw,
            CONNECTIONS_FILE=hsi_hw_connections_file,
            READOUT_PERIOD_US = hsi_readout_period,
            HSI_DEVICE_NAME = hsi_device_name,
            HSI_ENDPOINT_ADDRESS = hsi_endpoint_address,
            HSI_ENDPOINT_PARTITION = hsi_endpoint_partition,
            HSI_RE_MASK=hsi_re_mask,
            HSI_FE_MASK=hsi_fe_mask,
            HSI_INV_MASK=hsi_inv_mask,
            HSI_SOURCE=hsi_source,
            HSI_SOURCE_ID=hsi_source_id,
            TIMING_PARTITION=timing_partition_name,
            TIMING_HOST=host_timing,
            TIMING_PORT=port_timing,
            HOST=host_hsi,
            DEBUG=debug)
    else:
        the_system.apps["hsi"] = get_fake_hsi_app(
            CLOCK_SPEED_HZ = clock_speed_hz,
            DATA_RATE_SLOWDOWN_FACTOR = data_rate_slowdown_factor,
            TRIGGER_RATE_HZ = trigger_rate_hz,
            HSI_SOURCE_ID=hsi_source_id,
            MEAN_SIGNAL_MULTIPLICITY = mean_hsi_signal_multiplicity,
            SIGNAL_EMULATION_MODE = hsi_signal_emulation_mode,
            ENABLED_SIGNALS =  enabled_hsi_signals,
            HOST=host_hsi,
            DEBUG=debug)

        # the_system.apps["hsi"] = util.App(modulegraph=mgraph_hsi, host=host_hsi)
    if debug: console.log("hsi cmd data:", the_system.apps["hsi"])

    if control_timing_partition:
        the_system.apps["tprtc"] = get_tprtc_app(
            MASTER_DEVICE_NAME=timing_partition_master_device_name,
            TIMING_PARTITION_ID=timing_partition_id,
            TRIGGER_MASK=timing_partition_trigger_mask,
            RATE_CONTROL_ENABLED=timing_partition_rate_control_enabled,
            SPILL_GATE_ENABLED=timing_partition_spill_gate_enabled,
            TIMING_PARTITION=timing_partition_name,
            TIMING_HOST=host_timing,
            TIMING_PORT=port_timing,
            HOST=host_tprtc,
            DEBUG=debug)

    the_system.apps['trigger'] = get_trigger_app(
        DATA_RATE_SLOWDOWN_FACTOR = data_rate_slowdown_factor,
        CLOCK_SPEED_HZ = clock_speed_hz,
        TP_CONFIG = tp_infos,
        ACTIVITY_PLUGIN = trigger_activity_plugin,
        ACTIVITY_CONFIG = eval(trigger_activity_config),
        CANDIDATE_PLUGIN = trigger_candidate_plugin,
        CANDIDATE_CONFIG = eval(trigger_candidate_config),
        TTCM_S1=ttcm_s1,
        TTCM_S2=ttcm_s2,
        TRIGGER_WINDOW_BEFORE_TICKS = trigger_window_before_ticks,
        TRIGGER_WINDOW_AFTER_TICKS = trigger_window_after_ticks,
        HSI_TRIGGER_TYPE_PASSTHROUGH = hsi_trigger_type_passthrough,
	MLT_BUFFER_TIMEOUT = mlt_buffer_timeout,
        MLT_MAX_TD_LENGTH_MS = mlt_max_td_length_ms,
        MLT_SEND_TIMED_OUT_TDS = mlt_send_timed_out_tds,
        CHANNEL_MAP_NAME = tpg_channel_map,
        DATA_REQUEST_TIMEOUT=trigger_data_request_timeout,
        HOST=host_trigger,
        DEBUG=debug)

    the_system.apps['dfo'] = get_dfo_app(
        DF_COUNT = len(host_df),
        TOKEN_COUNT = trigemu_token_count,
        STOP_TIMEOUT = dfo_stop_timeout,
        HOST=host_dfo,
        DEBUG=debug)

        
    ru_app_names=[]
    dqm_app_names = []
    for dro_idx,dro_config in enumerate(dro_infos):
        host=dro_config.host.replace("-","")
        ru_name = f"ru{host}{dro_config.card}"
        ru_app_names.append(ru_name)
        the_system.apps[ru_name] = get_readout_app(
            HOST=dro_config.host,
            DRO_CONFIG=dro_config,
            EMULATOR_MODE = emulator_mode,
            DATA_RATE_SLOWDOWN_FACTOR = data_rate_slowdown_factor,
            DATA_FILE = data_file,
            FLX_INPUT = use_felix,
            CLOCK_SPEED_HZ = clock_speed_hz,
            RAW_RECORDING_ENABLED = enable_raw_recording,
            RAW_RECORDING_OUTPUT_DIR = raw_recording_output_dir,
            SOFTWARE_TPG_ENABLED = enable_software_tpg,
            FIRMWARE_TPG_ENABLED = enable_firmware_tpg,
            TPG_CHANNEL_MAP = tpg_channel_map,
            USE_FAKE_DATA_PRODUCERS = use_fake_data_producers,
            LATENCY_BUFFER_SIZE=latency_buffer_size,
            DATA_REQUEST_TIMEOUT=readout_data_request_timeout,
            SOURCEID_BROKER = sourceid_broker,
            READOUT_SENDS_TP_FRAGMENTS = readout_sends_tp_fragments,
            DEBUG=debug)
        if use_k8s:
            if use_felix:
                the_system.apps[ru_name].resources = {
                    "felix.cern/flx0-data": "1", # requesting FLX0
                    "memory": "32Gi" # yes bro
                }

        if debug:
            console.log(f"{ru_name} app: {the_system.apps[ru_name]}")

        if enable_dqm:
            dqm_name = "dqm_" + ru_name
            dqm_app_names.append(dqm_name)
            dqm_links = [link.dro_source_id for link in dro_config.links]
            the_system.apps[dqm_name] = get_dqm_app(
                DATA_RATE_SLOWDOWN_FACTOR = data_rate_slowdown_factor,
                CLOCK_SPEED_HZ = clock_speed_hz,
                DQMIDX = dro_idx,
                DQM_KAFKA_ADDRESS=dqm_kafka_address,
                DQM_CMAP=dqm_cmap,
                DQM_RAWDISPLAY_PARAMS=dqm_rawdisplay_params,
                DQM_MEANRMS_PARAMS=dqm_meanrms_params,
                DQM_FOURIER_PARAMS=dqm_fourier_params,
                DQM_FOURIERSUM_PARAMS=dqm_fouriersum_params,
                LINKS=dqm_links,
                HOST=host_dqm[dro_idx % len(host_dqm)],
                DEBUG=debug)

            if debug: console.log(f"{dqm_name} app: {the_system.apps[dqm_name]}")

    if enable_tpset_writing:
        tpw_name=f'tpwriter'
        the_system.apps[tpw_name] = get_tpwriter_app(
            OUTPUT_PATH = tpset_output_path,
            OPERATIONAL_ENVIRONMENT = op_env,
            MAX_FILE_SIZE = tpset_output_file_size,
            DATA_RATE_SLOWDOWN_FACTOR = data_rate_slowdown_factor,
            CLOCK_SPEED_HZ = clock_speed_hz,
            HARDWARE_MAP_FILE=hardware_map_file,
            HOST=host_tpw,
            DEBUG=debug)
        if use_k8s: ## TODO schema
            the_system.apps[tpw_name].mounted_dirs += [{
                'name': 'raw-data',
                'physical_location':output_path,
                'in_pod_location':output_path,
                'read_only': False
            }]

        if debug: console.log(f"{tpw_name} app: {the_system.apps[tpw_name]}")

    df_app_names = []
    dqm_df_app_names = []

    for app_name,df_config in appconfig_df.items():
        df_app_names.append(app_name)
        dfidx = sourceid_broker.get_next_source_id("TRBuilder")
        sourceid_broker.register_source_id("TRBuilder", dfidx, None)
        the_system.apps[app_name] = get_dataflow_app(
            HOSTIDX=dfidx,
            OUTPUT_PATHS = df_config["output_paths"],
            APP_NAME=app_name,
            OPERATIONAL_ENVIRONMENT = op_env,
            MAX_FILE_SIZE = df_config["max_file_size"],
            MAX_TRIGGER_RECORD_WINDOW = df_config["max_trigger_record_window"],
            MAX_EXPECTED_TR_SEQUENCES = max_expected_tr_sequences,
            TOKEN_COUNT = df_config["token_count"],
            TRB_TIMEOUT = trigger_record_building_timeout,
            HOST=df_config["host_df"],
            HAS_DQM=enable_dqm,
            HARDWARE_MAP_FILE=hardware_map_file,
            DEBUG=debug
        )
        if use_k8s:
            the_system.apps[app_name].mounted_dirs += [{
                'name': 'raw-data',
                'physical_location':df_config["output_paths"],
                'in_pod_location':df_config["output_paths"],
                'read_only': False,
            }]


        if enable_dqm:
            dqm_name = f"dqm_df{dfidx}"
            dqm_df_app_names.append(dqm_name)
            dqm_links = [link.dro_source_id for link in dro_config.links for dro_config in dro_infos]
            the_system.apps[dqm_name] = get_dqm_app(
                DATA_RATE_SLOWDOWN_FACTOR = data_rate_slowdown_factor,
                CLOCK_SPEED_HZ = clock_speed_hz,
                DQMIDX = dfidx,
                DQM_KAFKA_ADDRESS=dqm_kafka_address,
                DQM_CMAP=dqm_cmap,
                DQM_RAWDISPLAY_PARAMS=[0, 0],
                DQM_MEANRMS_PARAMS=[0, 0],
                DQM_FOURIER_PARAMS=[0, 0],
                DQM_FOURIERSUM_PARAMS=[0, 0],
                LINKS=dqm_links,
                HOST=host_dqm[dfidx%len(host_dqm)],
                MODE='df',
                DF_RATE=dqm_df_rate * len(host_df),
                DF_ALGS=dqm_df_algs,
                DF_TIME_WINDOW=trigger_window_before_ticks + trigger_window_after_ticks,
                FRONTEND_TYPE='',
                DEBUG=debug)

            if debug: console.log(f"{dqm_name} app: {the_system.apps[dqm_name]}")


    all_apps_except_ru = []
    all_apps_except_ru_and_df = []

    for name,app in the_system.apps.items():
        if app.name=="__app":
            app.name=name

        if app.name not in ru_app_names:
            all_apps_except_ru += [app]
        if app.name not in ru_app_names+df_app_names:
            all_apps_except_ru_and_df += [name]

        # HACK
        boot_order = ru_app_names + df_app_names + [app for app in all_apps_except_ru_and_df]
        if debug:
            console.log(f'Boot order: {boot_order}')

    #     console.log(f"MDAapp config generated in {json_dir}")
    from daqconf.core.conf_utils import make_app_command_data
    from daqconf.core.fragment_producers import  connect_all_fragment_producers, set_mlt_links, remove_mlt_link

    if debug:
        the_system.export("system_no_frag_prod_connection.dot")
    connect_all_fragment_producers(the_system, verbose=debug)

    # console.log("After connecting fragment producers, trigger mgraph:", the_system.apps['trigger'].modulegraph)
    # console.log("After connecting fragment producers, the_system.app_connections:", the_system.app_connections)

    set_mlt_links(the_system, "trigger", verbose=debug)

    mlt_links=the_system.apps["trigger"].modulegraph.get_module("mlt").conf.links
    if debug:
        console.log(f"After set_mlt_links, mlt_links is {mlt_links}")

    # HACK HACK HACK P. Rodrigues 2022-03-04 We decided not to request
    # TPs from readout for the 2.10 release. It would be nice to
    # achieve this by just not adding fragment producers for the
    # relevant links in readout_gen.py, but then the necessary input
    # and output queues for the DataLinkHandler modules are not
    # created. So instead we do it this roundabout way: the fragment
    # producers are all created, they are added to the MLT's list of
    # links to read out from (in set_mlt_links above), and then
    # removed here. We rely on a convention that TP links have element
    # value >= 1000.
    #
    # This code should be removed after 2.10, when we will have
    # decided how to handle raw TP data as fragments
#    for link in mlt_links:
#        if link["subsystem"] == system_type and link["element"] >= 1000:
#            remove_mlt_link(the_system, link)

    mlt_links=the_system.apps["trigger"].modulegraph.get_module("mlt").conf.links
    if debug:
        console.log(f"After remove_mlt_links, mlt_links is {mlt_links}")
    # END HACK

    if debug:
        the_system.export("system.dot")

    ####################################################################
    # Application command data generation
    ####################################################################

    # Arrange per-app command data into the format used by util.write_json_files()
    app_command_datas = {
        name : make_app_command_data(the_system, app,name, verbose=debug, use_k8s=use_k8s)
        for name,app in the_system.apps.items()
    }

    ##################################################################################

    # Make boot.json config
    from daqconf.core.conf_utils import make_system_command_datas,generate_boot_common, write_json_files

    # HACK: Make sure RUs start after trigger
    forced_deps = []

    for i,host in enumerate(dro_infos):
        ru_name = ru_app_names[i]
        forced_deps.append(['hsi', ru_name])
        if enable_tpset_writing:
            forced_deps.append(['tpwriter', ru_name])

    if enable_dqm:
        for i,host in enumerate(dro_infos):
            dqm_name = dqm_app_names[i]
            forced_deps.append([dqm_name, 'dfo'])
        for i,host in enumerate(host_df):
            dqm_name = dqm_df_app_names[i]
            forced_deps.append([dqm_name, 'dfo'])
    forced_deps.append(['trigger','hsi'])

    system_command_datas = make_system_command_datas(the_system, forced_deps, verbose=debug)

    external_connections = []
    for app in the_system.apps:
        external_connections += [ext.external_name for ext in the_system.apps[app].modulegraph.external_connections]

    # Override the default boot.json with the one from minidaqapp
    boot = generate_boot_common(
        ers_settings = ers_settings,
        info_svc_uri = info_svc_uri,
        disable_trace = disable_trace,
        use_kafka = use_kafka,
        external_connections = external_connections,
        daq_app_exec_name = "daq_application_ssh" if not use_k8s else "daq_application_k8s",
        verbose = debug,
    )

    if use_k8s:
        from daqconf.core.conf_utils import update_with_k8s_boot_data
        console.log("Generating k8s boot.json")
        update_with_k8s_boot_data(
            boot_data = boot,
            apps = the_system.apps,
            base_command_port = base_command_port,
            boot_order = boot_order,
            image = image,
            verbose = debug,
        )
    else:
        from daqconf.core.conf_utils import update_with_ssh_boot_data
        console.log("Generating ssh boot.json")
        update_with_ssh_boot_data(
            boot_data = boot,
            apps = the_system.apps,
            base_command_port = base_command_port,
            verbose = debug,
        )


    if thread_pinning_file != "" and exists(thread_pinning_file):
        boot['scripts'] = {
            "thread_pinning": {
                "cmd": [
                    "readout-affinity.py --pinfile ${DUNEDAQ_THREAD_PIN_FILE}"
                ],
                "env": {
                    "DUNEDAQ_THREAD_PIN_FILE": thread_pinning_file,
                    "LD_LIBRARY_PATH": "getenv",
                    "PATH": "getenv"
                }
            }
        }


    system_command_datas['boot'] = boot

    write_json_files(app_command_datas, system_command_datas, json_dir, verbose=debug)

    console.log(f"MDAapp config generated in {json_dir}")


    write_metadata_file(json_dir, "daqconf_multiru_gen", config_file)

if __name__ == '__main__':
    try:
        cli(show_default=True, standalone_mode=True)
    except Exception as e:
        console.print_exception()
